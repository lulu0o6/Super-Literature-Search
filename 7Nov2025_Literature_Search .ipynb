{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMc8+GQDmw/dPxlexMgapLn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lulu0o6/Super-Literature-Search/blob/main/7Nov2025_Literature_Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# ğŸ” Enhanced Literature Collector with 400+ High-Impact Journals\n",
        "# Keyword: GLP-1 RA depression\n",
        "# Date: 7Nov2025æ–‡çŒ®æ£€ç´¢\n",
        "# ===============================================\n",
        "\n",
        "!pip install requests beautifulsoup4 googletrans==4.0.0-rc1 python-docx tqdm scholarly feedparser\n",
        "\n",
        "import requests, re, datetime, urllib.parse, time, json, os, pickle\n",
        "from bs4 import BeautifulSoup\n",
        "from googletrans import Translator\n",
        "from docx import Document\n",
        "from docx.oxml import OxmlElement\n",
        "from docx.oxml.ns import qn\n",
        "from tqdm import tqdm\n",
        "import feedparser\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# ---------- CONFIG ----------\n",
        "KEYWORDS = [\"GLP-1\", \"CNS\"]\n",
        "LOGIC_OPERATOR = \"AND\"\n",
        "START_YEAR = 2020\n",
        "END_YEAR = 2025\n",
        "MAX_RESULTS_PER_SOURCE = 30\n",
        "ENTREZ_EMAIL = \"brovo02@gmail.com\"\n",
        "\n",
        "# æ—¥æœŸæ ¼å¼ï¼ˆ7Nov2025ï¼‰\n",
        "DATE_PREFIX = datetime.datetime.now().strftime(\"%-d%b%Y\")  # Windowsç³»ç»Ÿå¯èƒ½éœ€è¦ä¿®æ”¹\n",
        "FILE_BASENAME = f\"{DATE_PREFIX}æ–‡çŒ®æ£€ç´¢\"\n",
        "\n",
        "# åˆ›å»ºæ•°æ®ä¿å­˜ç›®å½•\n",
        "DATA_DIR = \"/content/literature_data\"\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    os.makedirs(DATA_DIR)\n",
        "\n",
        "# æ•°æ®ä¿å­˜æ–‡ä»¶è·¯å¾„\n",
        "SEARCH_RESULTS_FILE = f\"{DATA_DIR}/search_results.pkl\"\n",
        "FINAL_DATA_FILE = f\"{DATA_DIR}/final_data.pkl\"\n",
        "PROGRESS_FILE = f\"{DATA_DIR}/progress.json\"\n",
        "\n",
        "# æ‰©å±•çš„é«˜å½±å“å› å­æœŸåˆŠåˆ—è¡¨ï¼ˆ400+æœŸåˆŠï¼‰\n",
        "HIGH_IMPACT_JOURNALS = [\n",
        "    # === é¡¶çº§ç»¼åˆæœŸåˆŠ (30ä¸ª) ===\n",
        "    \"Nature\", \"Science\", \"Cell\", \"Lancet\", \"New England Journal of Medicine\",\n",
        "    \"JAMA\", \"BMJ\", \"Nature Medicine\", \"Nature Reviews Drug Discovery\",\n",
        "    \"Science Translational Medicine\", \"Nature Communications\", \"PNAS\",\n",
        "    \"Nature Reviews Immunology\", \"Nature Reviews Molecular Cell Biology\",\n",
        "    \"Cell Research\", \"Science Advances\", \"Nature Methods\", \"Nature Biotechnology\",\n",
        "    \"Nature Genetics\", \"Nature Cell Biology\", \"Nature Structural & Molecular Biology\",\n",
        "    \"Cell Stem Cell\", \"Cell Metabolism\", \"Molecular Cell\", \"Developmental Cell\",\n",
        "    \"Current Biology\", \"eLife\", \"PLOS Biology\", \"EMBO Journal\", \"EMBO Reports\",\n",
        "\n",
        "    # === ç²¾ç¥ç—…å­¦ä¸ç¥ç»ç§‘å­¦ (100ä¸ª) ===\n",
        "    \"Nature Neuroscience\", \"Molecular Psychiatry\", \"American Journal of Psychiatry\",\n",
        "    \"Journal of Clinical Psychiatry\", \"Neuropsychopharmacology\", \"Biological Psychiatry\",\n",
        "    \"JAMA Psychiatry\", \"World Psychiatry\", \"Schizophrenia Bulletin\",\n",
        "    \"Journal of Neuroscience\", \"Brain\", \"Neuron\", \"Nature Reviews Neuroscience\",\n",
        "    \"Lancet Psychiatry\", \"Lancet Neurology\", \"JAMA Neurology\", \"Annals of Neurology\",\n",
        "    \"Neurology\", \"Brain Stimulation\", \"Translational Psychiatry\", \"Cerebral Cortex\",\n",
        "    \"Journal of Neurochemistry\", \"Glia\", \"NeuroImage\", \"Human Brain Mapping\",\n",
        "    \"Biological Psychiatry: Cognitive Neuroscience and Neuroimaging\",\n",
        "    \"Neuropsychologia\", \"Journal of Cognitive Neuroscience\", \"Cognitive Neuropsychiatry\",\n",
        "    \"Depression and Anxiety\", \"Journal of Affective Disorders\", \"Bipolar Disorders\",\n",
        "    \"Acta Psychiatrica Scandinavica\", \"Psychological Medicine\", \"British Journal of Psychiatry\",\n",
        "    \"Journal of Psychiatric Research\", \"Psychiatry Research\", \"Progress in Neuro-Psychopharmacology\",\n",
        "    \"European Neuropsychopharmacology\", \"International Journal of Neuropsychopharmacology\",\n",
        "    \"Psychopharmacology\", \"Journal of Psychopharmacology\", \"Neuroscience\",\n",
        "    \"Neuroscience and Biobehavioral Reviews\", \"Frontiers in Neuroscience\",\n",
        "    \"Frontiers in Psychiatry\", \"Frontiers in Neurology\", \"Journal of Neurology\",\n",
        "    \"Multiple Sclerosis Journal\", \"Parkinsonism & Related Disorders\", \"Alzheimer's & Dementia\",\n",
        "    \"Journal of Alzheimer's Disease\", \"Neurobiology of Aging\", \"Neurobiology of Disease\",\n",
        "    \"Molecular Neurobiology\", \"Cellular and Molecular Neurobiology\", \"Neurotherapeutics\",\n",
        "    \"Experimental Neurology\", \"Journal of Neurotrauma\", \"Stroke\",\n",
        "    \"Annals of Clinical and Translational Neurology\", \"Journal of the Neurological Sciences\",\n",
        "    \"Clinical Neurophysiology\", \"Epilepsia\", \"Seizure\", \"Headache\",\n",
        "    \"Journal of Headache and Pain\", \"Pain\", \"European Journal of Pain\",\n",
        "    \"Journal of Pain\", \"Neuropsychology\", \"Journal of the International Neuropsychological Society\",\n",
        "    \"Archives of Clinical Neuropsychology\", \"Clinical Psychology Review\",\n",
        "    \"Journal of Abnormal Psychology\", \"Journal of Consulting and Clinical Psychology\",\n",
        "    \"Clinical Psychological Science\", \"Behaviour Research and Therapy\",\n",
        "    \"Journal of Behavior Therapy and Experimental Psychiatry\", \"Cognitive Therapy and Research\",\n",
        "    \"Mindfulness\", \"Journal of Contextual Behavioral Science\", \"Psychoneuroendocrinology\",\n",
        "    \"Stress\", \"Journal of Traumatic Stress\", \"European Archives of Psychiatry and Clinical Neuroscience\",\n",
        "    \"Australian and New Zealand Journal of Psychiatry\", \"Canadian Journal of Psychiatry\",\n",
        "    \"General Hospital Psychiatry\", \"Psychosomatic Medicine\", \"Journal of Psychosomatic Research\",\n",
        "    \"Psychotherapy and Psychosomatics\", \"Comprehensive Psychiatry\", \"Addiction\",\n",
        "    \"Drug and Alcohol Dependence\", \"Alcoholism: Clinical and Experimental Research\",\n",
        "\n",
        "    # === å†…åˆ†æ³Œä¸ä»£è°¢ (80ä¸ª) ===\n",
        "    \"Diabetes Care\", \"Journal of Clinical Endocrinology & Metabolism\",\n",
        "    \"Molecular Metabolism\", \"Diabetes\", \"Endocrine Reviews\", \"Cell Metabolism\",\n",
        "    \"Nature Reviews Endocrinology\", \"Lancet Diabetes & Endocrinology\",\n",
        "    \"Diabetologia\", \"Diabetes Obesity and Metabolism\", \"Metabolism: Clinical and Experimental\",\n",
        "    \"Endocrinology\", \"Journal of Endocrinology\", \"European Journal of Endocrinology\",\n",
        "    \"Clinical Endocrinology\", \"Hormone and Metabolic Research\", \"Thyroid\",\n",
        "    \"Journal of Thyroid Research\", \"Obesity\", \"International Journal of Obesity\",\n",
        "    \"Obesity Reviews\", \"American Journal of Physiology-Endocrinology and Metabolism\",\n",
        "    \"Frontiers in Endocrinology\", \"Journal of Diabetes Research\", \"Diabetes Technology & Therapeutics\",\n",
        "    \"Pediatric Diabetes\", \"Diabetes Research and Clinical Practice\", \"Journal of Diabetes\",\n",
        "    \"Journal of Diabetes and its Complications\", \"Current Diabetes Reports\",\n",
        "    \"Diabetes/Metabolism Research and Reviews\", \"Experimental and Clinical Endocrinology & Diabetes\",\n",
        "    \"Endocrine\", \"Endocrine Practice\", \"Journal of the Endocrine Society\",\n",
        "    \"Reviews in Endocrine and Metabolic Disorders\", \"Best Practice & Research Clinical Endocrinology & Metabolism\",\n",
        "    \"Molecular and Cellular Endocrinology\", \"Peptides\", \"Regulatory Peptides\",\n",
        "    \"Journal of Peptide Science\", \"Neuroendocrinology\", \"Psychoneuroendocrinology\",\n",
        "    \"Journal of Neuroendocrinology\", \"Frontiers in Neuroendocrinology\",\n",
        "    \"Growth Hormone & IGF Research\", \"Bone\", \"Journal of Bone and Mineral Research\",\n",
        "    \"Osteoporosis International\", \"Calcified Tissue International\", \"Bone Research\",\n",
        "    \"Journal of Clinical Densitometry\", \"Journal of Steroid Biochemistry and Molecular Biology\",\n",
        "    \"Steroids\", \"Molecular and Cellular Biochemistry\", \"Biochimica et Biophysica Acta (BBA) - Molecular Basis of Disease\",\n",
        "    \"Journal of Biological Chemistry\", \"Biochemical Journal\", \"FEBS Journal\",\n",
        "    \"Journal of Molecular Endocrinology\", \"Endocrine-Related Cancer\",\n",
        "    \"Journal of Clinical Oncology\", \"Cancer Research\", \"Clinical Cancer Research\",\n",
        "    \"Molecular Cancer Research\", \"Cancer Cell\", \"Nature Reviews Cancer\",\n",
        "    \"Journal of the National Cancer Institute\", \"Cancer Discovery\",\n",
        "    \"CA: A Cancer Journal for Clinicians\", \"Annals of Oncology\",\n",
        "    \"European Journal of Cancer\", \"British Journal of Cancer\",\n",
        "    \"International Journal of Cancer\", \"Carcinogenesis\",\n",
        "\n",
        "    # === è¯ç†å­¦ä¸è¯ç‰©å‘ç° (70ä¸ª) ===\n",
        "    \"Nature Reviews Drug Discovery\", \"Pharmacological Reviews\",\n",
        "    \"Clinical Pharmacology & Therapeutics\", \"British Journal of Pharmacology\",\n",
        "    \"Journal of Pharmacology and Experimental Therapeutics\", \"Molecular Pharmacology\",\n",
        "    \"Pharmacology & Therapeutics\", \"Trends in Pharmacological Sciences\",\n",
        "    \"Drug Discovery Today\", \"Expert Opinion on Drug Discovery\",\n",
        "    \"Journal of Medicinal Chemistry\", \"Bioorganic & Medicinal Chemistry\",\n",
        "    \"European Journal of Medicinal Chemistry\", \"Medicinal Research Reviews\",\n",
        "    \"Drug Metabolism Reviews\", \"Xenobiotica\", \"Clinical Pharmacokinetics\",\n",
        "    \"British Journal of Clinical Pharmacology\", \"Journal of Clinical Pharmacy and Therapeutics\",\n",
        "    \"Therapeutic Drug Monitoring\", \"Pharmacogenomics\", \"Pharmacogenetics and Genomics\",\n",
        "    \"Journal of Pharmacokinetics and Pharmacodynamics\", \"CPT: Pharmacometrics & Systems Pharmacology\",\n",
        "    \"AAPS Journal\", \"Pharmaceutical Research\", \"International Journal of Pharmaceutics\",\n",
        "    \"Journal of Controlled Release\", \"Advanced Drug Delivery Reviews\",\n",
        "    \"Molecular Pharmaceutics\", \"European Journal of Pharmaceutics and Biopharmaceutics\",\n",
        "    \"Journal of Pharmaceutical Sciences\", \"Pharmaceutics\", \"Drug Delivery\",\n",
        "    \"Expert Opinion on Drug Delivery\", \"Journal of Drug Targeting\",\n",
        "    \"Current Opinion in Pharmacology\", \"Biochemical Pharmacology\",\n",
        "    \"Cellular and Molecular Life Sciences\", \"Journal of Cellular Physiology\",\n",
        "    \"Journal of Biological Chemistry\", \"Proceedings of the National Academy of Sciences\",\n",
        "    \"ACS Chemical Neuroscience\", \"Journal of Neurochemistry\",\n",
        "    \"Neuropharmacology\", \"Psychopharmacology\", \"European Journal of Pharmacology\",\n",
        "    \"Naunyn-Schmiedeberg's Archives of Pharmacology\", \"Pharmacology Biochemistry and Behavior\",\n",
        "    \"Journal of Ethnopharmacology\", \"Planta Medica\", \"Phytomedicine\",\n",
        "    \"Journal of Natural Products\", \"Marine Drugs\", \"Frontiers in Pharmacology\",\n",
        "    \"Pharmacological Research\", \"Basic & Clinical Pharmacology & Toxicology\",\n",
        "    \"Toxicology and Applied Pharmacology\", \"Chemical Research in Toxicology\",\n",
        "    \"Expert Opinion on Therapeutic Patents\", \"Drugs\", \"CNS Drugs\",\n",
        "    \"Journal of Antimicrobial Chemotherapy\", \"Antimicrobial Agents and Chemotherapy\",\n",
        "    \"Journal of Infectious Diseases\", \"Clinical Infectious Diseases\",\n",
        "    \"Vaccine\", \"Expert Review of Vaccines\", \"mAbs\", \"Journal of Immunotherapy\",\n",
        "\n",
        "    # === å¿ƒè¡€ç®¡ç§‘å­¦ (60ä¸ª) ===\n",
        "    \"Journal of the American College of Cardiology\", \"Circulation\",\n",
        "    \"European Heart Journal\", \"Circulation Research\", \"Journal of the American Heart Association\",\n",
        "    \"Arteriosclerosis, Thrombosis, and Vascular Biology\", \"Hypertension\",\n",
        "    \"Journal of Hypertension\", \"American Journal of Hypertension\",\n",
        "    \"European Journal of Heart Failure\", \"Journal of Cardiac Failure\",\n",
        "    \"Heart\", \"Journal of the American Society of Echocardiography\",\n",
        "    \"European Journal of Echocardiography\", \"JACC: Cardiovascular Imaging\",\n",
        "    \"JACC: Heart Failure\", \"JACC: Clinical Electrophysiology\",\n",
        "    \"Europace\", \"Heart Rhythm\", \"Pacing and Clinical Electrophysiology\",\n",
        "    \"Cardiovascular Research\", \"Basic Research in Cardiology\",\n",
        "    \"American Journal of Physiology-Heart and Circulatory Physiology\",\n",
        "    \"Journal of Molecular and Cellular Cardiology\", \"Cardiovascular Diabetology\",\n",
        "    \"Diabetes and Vascular Disease Research\", \"Atherosclerosis\",\n",
        "    \"Journal of Lipid Research\", \"Arteriosclerosis\",\n",
        "    \"Current Opinion in Lipidology\", \"Progress in Lipid Research\",\n",
        "    \"Clinical Science\", \"American Journal of Cardiology\",\n",
        "    \"International Journal of Cardiology\", \"Canadian Journal of Cardiology\",\n",
        "    \"Journal of Cardiovascular Electrophysiology\", \"Circulation: Arrhythmia and Electrophysiology\",\n",
        "    \"Circulation: Cardiovascular Imaging\", \"Circulation: Cardiovascular Interventions\",\n",
        "    \"Circulation: Heart Failure\", \"Circulation: Genomic and Precision Medicine\",\n",
        "    \"Nature Reviews Cardiology\", \"Lancet Cardiology\",\n",
        "    \"JACC: Basic to Translational Science\", \"Cardiovascular Drugs and Therapy\",\n",
        "    \"Journal of Cardiovascular Pharmacology\", \"Pharmacological Research\",\n",
        "    \"Frontiers in Cardiovascular Medicine\", \"European Journal of Preventive Cardiology\",\n",
        "    \"Journal of Clinical Lipidology\", \"Nutrition, Metabolism and Cardiovascular Diseases\",\n",
        "    \"Obesity Research & Clinical Practice\", \"Journal of Thoracic and Cardiovascular Surgery\",\n",
        "    \"Annals of Thoracic Surgery\", \"European Journal of Cardio-Thoracic Surgery\",\n",
        "    \"Journal of Vascular Surgery\", \"European Journal of Vascular and Endovascular Surgery\",\n",
        "\n",
        "    # === åˆ†å­ç”Ÿç‰©å­¦ä¸ç»†èƒç”Ÿç‰©å­¦ (60ä¸ª) ===\n",
        "    \"Molecular Cell\", \"Cell Reports\", \"Science Advances\", \"Nature Metabolism\",\n",
        "    \"JCI Insight\", \"Cell Research\", \"EMBO Journal\", \"Genes & Development\",\n",
        "    \"Developmental Cell\", \"Current Biology\", \"eLife\", \"PLOS Biology\",\n",
        "    \"Journal of Cell Biology\", \"Molecular Biology of the Cell\", \"Journal of Cell Science\",\n",
        "    \"Cell Death & Differentiation\", \"Autophagy\", \"Apoptosis\", \"Journal of Biological Chemistry\",\n",
        "    \"Biochemical Journal\", \"FEBS Letters\", \"Biochimica et Biophysica Acta (BBA)\",\n",
        "    \"Journal of Molecular Biology\", \"Structure\", \"Journal of Structural Biology\",\n",
        "    \"Current Opinion in Cell Biology\", \"Trends in Cell Biology\", \"Nature Cell Biology\",\n",
        "    \"Cell Cycle\", \"Cell Signal\", \"Cellular Signalling\", \"Journal of Cellular Biochemistry\",\n",
        "    \"Experimental Cell Research\", \"Cell and Tissue Research\", \"Histochemistry and Cell Biology\",\n",
        "    \"Journal of Histochemistry and Cytochemistry\", \"Cytokine\", \"Growth Factors\",\n",
        "    \"Stem Cells\", \"Stem Cell Reports\", \"Cell Stem Cell\", \"Nature Stem Cell\",\n",
        "    \"Stem Cell Research & Therapy\", \"Journal of Stem Cells and Regenerative Medicine\",\n",
        "    \"Tissue Engineering\", \"Biomaterials\", \"Acta Biomaterialia\",\n",
        "    \"Journal of Biomedical Materials Research\", \"Advanced Healthcare Materials\",\n",
        "    \"Biofabrication\", \"Organogenesis\", \"Developmental Biology\",\n",
        "    \"Genesis\", \"Mechanisms of Development\", \"Differentiation\",\n",
        "    \"Seminars in Cell & Developmental Biology\", \"International Journal of Developmental Biology\",\n",
        "    \"Journal of Developmental Biology\", \"Frontiers in Cell and Developmental Biology\"\n",
        "]\n",
        "\n",
        "# æ—¥æœŸæ ¼å¼\n",
        "translator = Translator()\n",
        "\n",
        "# ---------- DATA PERSISTENCE FUNCTIONS ----------\n",
        "def save_progress(stage, data=None):\n",
        "    \"\"\"ä¿å­˜è¿›åº¦\"\"\"\n",
        "    progress = {\n",
        "        'stage': stage,\n",
        "        'timestamp': datetime.datetime.now().isoformat(),\n",
        "        'data_size': len(data) if data else 0\n",
        "    }\n",
        "    with open(PROGRESS_FILE, 'w') as f:\n",
        "        json.dump(progress, f)\n",
        "\n",
        "    if data is not None:\n",
        "        if stage == 'search_complete':\n",
        "            with open(SEARCH_RESULTS_FILE, 'wb') as f:\n",
        "                pickle.dump(data, f)\n",
        "        elif stage == 'deduplication_complete':\n",
        "            with open(FINAL_DATA_FILE, 'wb') as f:\n",
        "                pickle.dump(data, f)\n",
        "\n",
        "    print(f\"ğŸ’¾ Progress saved: {stage}\")\n",
        "\n",
        "def load_progress():\n",
        "    \"\"\"åŠ è½½è¿›åº¦\"\"\"\n",
        "    if os.path.exists(PROGRESS_FILE):\n",
        "        with open(PROGRESS_FILE, 'r') as f:\n",
        "            return json.load(f)\n",
        "    return None\n",
        "\n",
        "def load_saved_data():\n",
        "    \"\"\"åŠ è½½å·²ä¿å­˜çš„æ•°æ®\"\"\"\n",
        "    data = {}\n",
        "    if os.path.exists(SEARCH_RESULTS_FILE):\n",
        "        with open(SEARCH_RESULTS_FILE, 'rb') as f:\n",
        "            data['search_results'] = pickle.load(f)\n",
        "    if os.path.exists(FINAL_DATA_FILE):\n",
        "        with open(FINAL_DATA_FILE, 'rb') as f:\n",
        "            data['final_data'] = pickle.load(f)\n",
        "    return data\n",
        "\n",
        "def check_existing_data():\n",
        "    \"\"\"æ£€æŸ¥æ˜¯å¦æœ‰å·²ä¿å­˜çš„æ•°æ®\"\"\"\n",
        "    progress = load_progress()\n",
        "    if progress:\n",
        "        print(f\"ğŸ“ Found existing progress: {progress['stage']} at {progress['timestamp']}\")\n",
        "\n",
        "        saved_data = load_saved_data()\n",
        "        if 'search_results' in saved_data:\n",
        "            print(f\"   Search results: {len(saved_data['search_results'])} items\")\n",
        "        if 'final_data' in saved_data:\n",
        "            print(f\"   Final data: {len(saved_data['final_data'])} items\")\n",
        "\n",
        "        return progress, saved_data\n",
        "    return None, {}\n",
        "\n",
        "# ---------- KEYWORD FILTERING FUNCTIONS ----------\n",
        "def preprocess_text(text):\n",
        "    \"\"\"é¢„å¤„ç†æ–‡æœ¬ç”¨äºå…³é”®è¯åŒ¹é…\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    # è½¬æ¢ä¸ºå°å†™ï¼Œç§»é™¤æ ‡ç‚¹\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def calculate_keyword_relevance(title, abstract, keywords):\n",
        "    \"\"\"è®¡ç®—æ–‡çŒ®ä¸å…³é”®è¯çš„ç›¸å…³æ€§åˆ†æ•°\"\"\"\n",
        "    if not title or not abstract:\n",
        "        return 0\n",
        "\n",
        "    text = preprocess_text(title + \" \" + abstract)\n",
        "    if not text:\n",
        "        return 0\n",
        "\n",
        "    # è®¡ç®—å…³é”®è¯å‡ºç°é¢‘ç‡å’Œä½ç½®æƒé‡\n",
        "    relevance_score = 0\n",
        "\n",
        "    for keyword in keywords:\n",
        "        keyword_lower = keyword.lower()\n",
        "\n",
        "        # åœ¨æ ‡é¢˜ä¸­å‡ºç°ï¼ˆæœ€é«˜æƒé‡ï¼‰\n",
        "        title_lower = preprocess_text(title)\n",
        "        title_matches = len(re.findall(r'\\b' + re.escape(keyword_lower) + r'\\b', title_lower))\n",
        "        relevance_score += title_matches * 3\n",
        "\n",
        "        # åœ¨æ‘˜è¦ä¸­å‡ºç°\n",
        "        abstract_lower = preprocess_text(abstract)\n",
        "        abstract_matches = len(re.findall(r'\\b' + re.escape(keyword_lower) + r'\\b', abstract_lower))\n",
        "        relevance_score += abstract_matches * 1\n",
        "\n",
        "        # éƒ¨åˆ†åŒ¹é…ï¼ˆæ¬¡æƒé‡ï¼‰\n",
        "        if keyword_lower in text:\n",
        "            relevance_score += 0.5\n",
        "\n",
        "    return relevance_score\n",
        "\n",
        "def filter_by_keywords(items, keywords, min_relevance_score=2):\n",
        "    \"\"\"æ ¹æ®å…³é”®è¯ç­›é€‰æ–‡çŒ®\"\"\"\n",
        "    print(f\"\\nğŸ” Filtering {len(items)} items by keywords: {keywords}\")\n",
        "\n",
        "    filtered_items = []\n",
        "    relevance_scores = []\n",
        "\n",
        "    for item in tqdm(items, desc=\"Keyword Filtering\"):\n",
        "        title = item.get(\"Title\", \"\")\n",
        "        abstract = item.get(\"Abstract_En\", \"\")\n",
        "\n",
        "        relevance = calculate_keyword_relevance(title, abstract, keywords)\n",
        "        relevance_scores.append(relevance)\n",
        "\n",
        "        if relevance >= min_relevance_score:\n",
        "            item[\"Relevance_Score\"] = relevance\n",
        "            filtered_items.append(item)\n",
        "\n",
        "    # ç»Ÿè®¡ä¿¡æ¯\n",
        "    total_items = len(items)\n",
        "    filtered_count = len(filtered_items)\n",
        "    removed_count = total_items - filtered_count\n",
        "\n",
        "    print(f\"ğŸ“Š KEYWORD FILTERING RESULTS:\")\n",
        "    print(f\"  Total items: {total_items}\")\n",
        "    print(f\"  After filtering: {filtered_count}\")\n",
        "    print(f\"  Removed: {removed_count} ({removed_count/total_items*100:.1f}%)\")\n",
        "\n",
        "    # æ˜¾ç¤ºç›¸å…³æ€§åˆ†æ•°åˆ†å¸ƒ\n",
        "    if relevance_scores:\n",
        "        avg_score = sum(relevance_scores) / len(relevance_scores)\n",
        "        max_score = max(relevance_scores)\n",
        "        min_score = min(relevance_scores)\n",
        "        print(f\"  Relevance scores - Avg: {avg_score:.2f}, Max: {max_score}, Min: {min_score}\")\n",
        "\n",
        "    # æ˜¾ç¤ºè¢«è¿‡æ»¤æ‰çš„æ–‡çŒ®ç¤ºä¾‹\n",
        "    if removed_count > 0:\n",
        "        print(f\"\\nğŸ“ Examples of removed items (low relevance):\")\n",
        "        low_relevance_items = [item for item, score in zip(items, relevance_scores) if score < min_relevance_score]\n",
        "        for i, item in enumerate(low_relevance_items[:3]):\n",
        "            print(f\"  {i+1}. {item['Title'][:80]}... (score: {relevance_scores[items.index(item)]})\")\n",
        "\n",
        "    return filtered_items\n",
        "\n",
        "def smart_keyword_expansion(keywords):\n",
        "    \"\"\"æ™ºèƒ½æ‰©å±•å…³é”®è¯ï¼ˆåŒä¹‰è¯ã€ç›¸å…³æœ¯è¯­ï¼‰\"\"\"\n",
        "    expanded_keywords = set(keywords)\n",
        "\n",
        "    # GLP-1 ç›¸å…³æœ¯è¯­\n",
        "    glp1_related = [\n",
        "        \"glp1\", \"glp-1\", \"glp 1\", \"glucagon-like peptide-1\", \"glucagon like peptide 1\",\n",
        "        \"glp-1 receptor\", \"glp1 receptor\", \"glp-1 ra\", \"glp1 ra\",\n",
        "        \"glp-1 agonist\", \"glp1 agonist\", \"glp-1 analog\", \"glp1 analog\",\n",
        "        \"semaglutide\", \"liraglutide\", \"dulaglutide\", \"exenatide\", \"lixisenatide\"\n",
        "    ]\n",
        "\n",
        "    # æŠ‘éƒç›¸å…³æœ¯è¯­\n",
        "    depression_related = [\n",
        "        \"depression\", \"depressive\", \"major depressive disorder\", \"mdd\",\n",
        "        \"bipolar\", \"bipolar disorder\", \"mood disorder\", \"affective disorder\",\n",
        "        \"mental health\", \"psychological\", \"antidepressant\", \"depression treatment\"\n",
        "    ]\n",
        "\n",
        "    # æ ¹æ®è¾“å…¥å…³é”®è¯è‡ªåŠ¨æ‰©å±•\n",
        "    for keyword in keywords:\n",
        "        keyword_lower = keyword.lower()\n",
        "\n",
        "        if any(term in keyword_lower for term in ['glp', 'peptide']):\n",
        "            expanded_keywords.update(glp1_related)\n",
        "        elif any(term in keyword_lower for term in ['depress', 'bipolar', 'mood']):\n",
        "            expanded_keywords.update(depression_related)\n",
        "\n",
        "    # ç§»é™¤åŸå§‹å…³é”®è¯ä¸­å¯èƒ½é‡å¤çš„\n",
        "    expanded_list = list(expanded_keywords)\n",
        "    print(f\"ğŸ”¤ Expanded keywords: {len(expanded_list)} terms\")\n",
        "    print(f\"   Original: {keywords}\")\n",
        "    print(f\"   Expanded: {expanded_list}\")\n",
        "\n",
        "    return expanded_list\n",
        "\n",
        "def analyze_keyword_distribution(items, keywords):\n",
        "    \"\"\"åˆ†æå…³é”®è¯åœ¨æ–‡çŒ®ä¸­çš„åˆ†å¸ƒ\"\"\"\n",
        "    print(f\"\\nğŸ“ˆ Analyzing keyword distribution in {len(items)} items...\")\n",
        "\n",
        "    keyword_stats = {}\n",
        "    for keyword in keywords:\n",
        "        keyword_lower = keyword.lower()\n",
        "        count = 0\n",
        "\n",
        "        for item in items:\n",
        "            text = preprocess_text(item.get(\"Title\", \"\") + \" \" + item.get(\"Abstract_En\", \"\"))\n",
        "            if keyword_lower in text:\n",
        "                count += 1\n",
        "\n",
        "        keyword_stats[keyword] = count\n",
        "\n",
        "    # æ˜¾ç¤ºå…³é”®è¯ç»Ÿè®¡\n",
        "    print(\"ğŸ“Š Keyword occurrence statistics:\")\n",
        "    for keyword, count in sorted(keyword_stats.items(), key=lambda x: x[1], reverse=True):\n",
        "        percentage = (count / len(items)) * 100 if items else 0\n",
        "        print(f\"   {keyword}: {count} items ({percentage:.1f}%)\")\n",
        "\n",
        "    return keyword_stats\n",
        "\n",
        "# ---------- UTILITIES ----------\n",
        "def clean_html(text):\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    text = re.sub(r'<[^>]+>', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def translate_to_cn(text):\n",
        "    try:\n",
        "        if not text.strip():\n",
        "            return \"\"\n",
        "        translated = translator.translate(text, src='en', dest='zh-cn').text\n",
        "        return translated\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Translation failed: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def make_hyperlink(paragraph, url, text):\n",
        "    \"\"\"åœ¨ Word æ®µè½ä¸­æ’å…¥å¯ç‚¹å‡»çš„è¶…é“¾æ¥\"\"\"\n",
        "    part = paragraph.part\n",
        "    r_id = part.relate_to(url, \"http://schemas.openxmlformats.org/officeDocument/2006/relationships/hyperlink\", is_external=True)\n",
        "    hyperlink = OxmlElement('w:hyperlink')\n",
        "    hyperlink.set(qn('r:id'), r_id)\n",
        "    new_run = OxmlElement('w:r')\n",
        "    rPr = OxmlElement('w:rPr')\n",
        "    rStyle = OxmlElement('w:rStyle')\n",
        "    rStyle.set(qn('w:val'), 'Hyperlink')\n",
        "    rPr.append(rStyle)\n",
        "    new_run.append(rPr)\n",
        "    text_elem = OxmlElement('w:t')\n",
        "    text_elem.text = text\n",
        "    new_run.append(text_elem)\n",
        "    hyperlink.append(new_run)\n",
        "    paragraph._p.append(hyperlink)\n",
        "\n",
        "def doi_to_link(doi):\n",
        "    return f\"https://doi.org/{doi}\" if doi else \"\"\n",
        "\n",
        "# ---------- ADVANCED DEDUPLICATION FUNCTIONS ----------\n",
        "def normalize_text(text):\n",
        "    \"\"\"æ ‡å‡†åŒ–æ–‡æœ¬ç”¨äºæ¯”è¾ƒ\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def calculate_similarity(title1, title2):\n",
        "    \"\"\"è®¡ç®—ä¸¤ä¸ªæ ‡é¢˜çš„ç›¸ä¼¼åº¦\"\"\"\n",
        "    norm1 = normalize_text(title1)\n",
        "    norm2 = normalize_text(title2)\n",
        "\n",
        "    if not norm1 or not norm2:\n",
        "        return 0\n",
        "\n",
        "    similarity = SequenceMatcher(None, norm1, norm2).ratio()\n",
        "\n",
        "    if len(norm1) < 20 or len(norm2) < 20:\n",
        "        return similarity * 0.8\n",
        "\n",
        "    return similarity\n",
        "\n",
        "def is_duplicate(item1, item2, similarity_threshold=0.85):\n",
        "    \"\"\"åˆ¤æ–­ä¸¤ä¸ªæ–‡çŒ®æ˜¯å¦é‡å¤\"\"\"\n",
        "    if item1[\"DOI\"] and item2[\"DOI\"] and item1[\"DOI\"] == item2[\"DOI\"]:\n",
        "        return True\n",
        "\n",
        "    if item1[\"Link\"] and item2[\"Link\"] and item1[\"Link\"] == item2[\"Link\"]:\n",
        "        return True\n",
        "\n",
        "    title_similarity = calculate_similarity(item1[\"Title\"], item2[\"Title\"])\n",
        "    if title_similarity > similarity_threshold:\n",
        "        return True\n",
        "\n",
        "    authors1 = normalize_text(item1[\"Authors\"])\n",
        "    authors2 = normalize_text(item2[\"Authors\"])\n",
        "    if (authors1 and authors2 and\n",
        "        len(authors1) > 10 and len(authors2) > 10 and\n",
        "        authors1 == authors2 and\n",
        "        item1[\"Year\"] == item2[\"Year\"]):\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def advanced_deduplication(items):\n",
        "    \"\"\"é«˜çº§å»é‡ç®—æ³•\"\"\"\n",
        "    print(f\"\\nğŸ§© Performing advanced deduplication on {len(items)} items...\")\n",
        "\n",
        "    exact_duplicates_removed = 0\n",
        "    unique_items = []\n",
        "    seen_keys = set()\n",
        "\n",
        "    for item in items:\n",
        "        doi_key = item[\"DOI\"] if item[\"DOI\"] else \"\"\n",
        "        title_key = normalize_text(item[\"Title\"])[:100] if item[\"Title\"] else \"\"\n",
        "        author_year_key = f\"{normalize_text(item['Authors'])[:50]}_{item['Year']}\" if item[\"Authors\"] else \"\"\n",
        "\n",
        "        if doi_key:\n",
        "            key = doi_key\n",
        "        elif title_key and author_year_key:\n",
        "            key = f\"{title_key}_{author_year_key}\"\n",
        "        else:\n",
        "            key = title_key\n",
        "\n",
        "        if key and key not in seen_keys:\n",
        "            seen_keys.add(key)\n",
        "            unique_items.append(item)\n",
        "        else:\n",
        "            exact_duplicates_removed += 1\n",
        "\n",
        "    print(f\"âœ… Exact duplicates removed: {exact_duplicates_removed}\")\n",
        "    print(f\"ğŸ“Š After exact deduplication: {len(unique_items)} items\")\n",
        "\n",
        "    similarity_duplicates_removed = 0\n",
        "    final_items = []\n",
        "    processed_indices = set()\n",
        "\n",
        "    for i in tqdm(range(len(unique_items)), desc=\"Similarity check\"):\n",
        "        if i in processed_indices:\n",
        "            continue\n",
        "\n",
        "        current_item = unique_items[i]\n",
        "        final_items.append(current_item)\n",
        "        processed_indices.add(i)\n",
        "\n",
        "        for j in range(i + 1, len(unique_items)):\n",
        "            if j in processed_indices:\n",
        "                continue\n",
        "\n",
        "            if is_duplicate(current_item, unique_items[j]):\n",
        "                processed_indices.add(j)\n",
        "                similarity_duplicates_removed += 1\n",
        "                if (len(unique_items[j][\"Abstract_En\"]) > len(current_item[\"Abstract_En\"]) and\n",
        "                    unique_items[j][\"Abstract_En\"] != \"Abstract not available from any source.\"):\n",
        "                    final_items[-1] = unique_items[j]\n",
        "\n",
        "    print(f\"âœ… Similarity duplicates removed: {similarity_duplicates_removed}\")\n",
        "    print(f\"ğŸ“Š After similarity deduplication: {len(final_items)} items\")\n",
        "\n",
        "    total_removed = exact_duplicates_removed + similarity_duplicates_removed\n",
        "    print(f\"ğŸ¯ Total duplicates removed: {total_removed}\")\n",
        "\n",
        "    return final_items\n",
        "\n",
        "def select_best_version(items):\n",
        "    \"\"\"ä»é‡å¤é¡¹ç›®ä¸­é€‰æ‹©æœ€ä½³ç‰ˆæœ¬\"\"\"\n",
        "    if not items:\n",
        "        return None\n",
        "\n",
        "    best_item = items[0]\n",
        "\n",
        "    for item in items[1:]:\n",
        "        if item[\"DOI\"] and not best_item[\"DOI\"]:\n",
        "            best_item = item\n",
        "            continue\n",
        "\n",
        "        if (len(item[\"Abstract_En\"]) > len(best_item[\"Abstract_En\"]) and\n",
        "            item[\"Abstract_En\"] != \"Abstract not available from any source.\"):\n",
        "            best_item = item\n",
        "            continue\n",
        "\n",
        "        source_priority = {\n",
        "            \"High-Impact\": 5,\n",
        "            \"PubMed\": 4,\n",
        "            \"CrossRef\": 3,\n",
        "            \"SemanticScholar\": 2,\n",
        "            \"Springer\": 2\n",
        "        }\n",
        "\n",
        "        current_priority = source_priority.get(best_item[\"Source\"], 1)\n",
        "        new_priority = source_priority.get(item[\"Source\"], 1)\n",
        "\n",
        "        if new_priority > current_priority:\n",
        "            best_item = item\n",
        "\n",
        "    return best_item\n",
        "\n",
        "def group_and_deduplicate(items):\n",
        "    \"\"\"åˆ†ç»„å¹¶å»é‡\"\"\"\n",
        "    print(f\"\\nğŸ§© Grouping and deduplicating {len(items)} items...\")\n",
        "\n",
        "    doi_groups = {}\n",
        "    no_doi_items = []\n",
        "\n",
        "    for item in items:\n",
        "        if item[\"DOI\"]:\n",
        "            if item[\"DOI\"] not in doi_groups:\n",
        "                doi_groups[item[\"DOI\"]] = []\n",
        "            doi_groups[item[\"DOI\"]].append(item)\n",
        "        else:\n",
        "            no_doi_items.append(item)\n",
        "\n",
        "    deduplicated_items = []\n",
        "    for doi, group in doi_groups.items():\n",
        "        best_item = select_best_version(group)\n",
        "        if best_item:\n",
        "            deduplicated_items.append(best_item)\n",
        "\n",
        "    print(f\"âœ… DOI-based groups: {len(doi_groups)}\")\n",
        "    print(f\"ğŸ“Š Items with DOI: {len(deduplicated_items)}\")\n",
        "    print(f\"ğŸ“Š Items without DOI: {len(no_doi_items)}\")\n",
        "\n",
        "    if no_doi_items:\n",
        "        no_doi_dedup = advanced_deduplication(no_doi_items)\n",
        "        deduplicated_items.extend(no_doi_dedup)\n",
        "\n",
        "    return deduplicated_items\n",
        "\n",
        "# ---------- ENHANCED ABSTRACT COMPLETION FUNCTIONS ----------\n",
        "def extract_abstract_from_doi_webpage(doi):\n",
        "    \"\"\"ä»DOIåŸç½‘é¡µæŠ“å–æ‘˜è¦\"\"\"\n",
        "    try:\n",
        "        doi_url = f\"https://doi.org/{doi}\"\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Accept-Encoding': 'gzip, deflate',\n",
        "            'Connection': 'keep-alive',\n",
        "            'Upgrade-Insecure-Requests': '1',\n",
        "        }\n",
        "\n",
        "        response = requests.get(doi_url, headers=headers, timeout=15)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # å¸¸è§æ‘˜è¦é€‰æ‹©å™¨ï¼ˆè¦†ç›–å¤§å¤šæ•°å‡ºç‰ˆç¤¾ï¼‰\n",
        "        abstract_selectors = [\n",
        "            # ScienceDirect\n",
        "            'div.abstract.author',\n",
        "            'section.abstract',\n",
        "            'div.Abstracts',\n",
        "            # Nature\n",
        "            'div.c-article-section__content',\n",
        "            'div#Abs1-content',\n",
        "            # Wiley\n",
        "            'section.article-section__abstract',\n",
        "            'div.abstract-group',\n",
        "            # Springer\n",
        "            'div.c-article-section__content',\n",
        "            'section.Abstract',\n",
        "            # PubMed Central\n",
        "            'div.abstract',\n",
        "            'div.abstract-content',\n",
        "            # PLOS\n",
        "            'div.abstract-content',\n",
        "            # BMC\n",
        "            'div.Abstract',\n",
        "            # é€šç”¨é€‰æ‹©å™¨\n",
        "            '[class*=\"abstract\"]',\n",
        "            '[id*=\"abstract\"]',\n",
        "            'section#abstract',\n",
        "            'div.abstract-section',\n",
        "            'p.abstract',\n",
        "        ]\n",
        "\n",
        "        # å°è¯•å„ç§é€‰æ‹©å™¨\n",
        "        for selector in abstract_selectors:\n",
        "            abstract_elements = soup.select(selector)\n",
        "            for element in abstract_elements:\n",
        "                text = element.get_text(strip=True)\n",
        "                if text and len(text) > 100:  # ç¡®ä¿æ˜¯çœŸæ­£çš„æ‘˜è¦\n",
        "                    # æ¸…ç†æ–‡æœ¬\n",
        "                    text = clean_html(text)\n",
        "                    if len(text) > 150:\n",
        "                        print(f\"    âœ… Found abstract via selector: {selector}\")\n",
        "                        return text\n",
        "\n",
        "        # å¦‚æœæ²¡æœ‰é€šè¿‡é€‰æ‹©å™¨æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾åŒ…å«\"Abstract\"çš„æ®µè½\n",
        "        abstract_keywords = ['abstract', 'summary', 'èƒŒæ™¯', 'æ‘˜è¦']\n",
        "        for keyword in abstract_keywords:\n",
        "            elements = soup.find_all(['div', 'section', 'p'],\n",
        "                                   string=re.compile(keyword, re.IGNORECASE))\n",
        "            for element in elements:\n",
        "                # è·å–ç›¸é‚»çš„æ–‡æœ¬å†…å®¹\n",
        "                next_sibling = element.find_next_sibling()\n",
        "                if next_sibling:\n",
        "                    text = next_sibling.get_text(strip=True)\n",
        "                    if text and len(text) > 100:\n",
        "                        text = clean_html(text)\n",
        "                        if len(text) > 150:\n",
        "                            print(f\"    âœ… Found abstract via keyword: {keyword}\")\n",
        "                            return text\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    âš ï¸ DOI webpage extraction failed: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def get_publisher_abstract(doi):\n",
        "    \"\"\"é€šè¿‡å‡ºç‰ˆç¤¾APIè·å–æ‘˜è¦\"\"\"\n",
        "    try:\n",
        "        # CrossRef API\n",
        "        crossref_url = f\"https://api.crossref.org/works/{doi}\"\n",
        "        response = requests.get(crossref_url, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            abstract = data.get('message', {}).get('abstract')\n",
        "            if abstract:\n",
        "                return clean_html(abstract)\n",
        "\n",
        "        # Semantic Scholar API\n",
        "        semantic_url = f\"https://api.semanticscholar.org/graph/v1/paper/DOI:{doi}?fields=abstract\"\n",
        "        response = requests.get(semantic_url, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            abstract = data.get('abstract')\n",
        "            if abstract:\n",
        "                return clean_html(abstract)\n",
        "\n",
        "        return \"\"\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "def get_pubmed_abstract(title):\n",
        "    \"\"\"é€šè¿‡æ ‡é¢˜åœ¨PubMedä¸­æœç´¢å¹¶è·å–æ‘˜è¦\"\"\"\n",
        "    try:\n",
        "        clean_title = re.sub(r'[^\\w\\s]', '', title)\n",
        "        search_url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term={urllib.parse.quote(clean_title)}&retmode=json\"\n",
        "\n",
        "        response = requests.get(search_url, timeout=10)\n",
        "        data = response.json()\n",
        "\n",
        "        id_list = data.get(\"esearchresult\", {}).get(\"idlist\", [])\n",
        "        if not id_list:\n",
        "            return \"\"\n",
        "\n",
        "        fetch_url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pubmed&id={id_list[0]}&retmode=xml\"\n",
        "        fetch_response = requests.get(fetch_url, timeout=10)\n",
        "        soup = BeautifulSoup(fetch_response.text, \"xml\")\n",
        "\n",
        "        abstract_texts = soup.find_all(\"AbstractText\")\n",
        "        if abstract_texts:\n",
        "            return clean_html(\" \".join([abs_text.text for abs_text in abstract_texts]))\n",
        "\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ PubMed abstract search failed: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def get_crossref_abstract(title):\n",
        "    \"\"\"é€šè¿‡æ ‡é¢˜åœ¨CrossRefä¸­æœç´¢æ‘˜è¦\"\"\"\n",
        "    try:\n",
        "        q = urllib.parse.quote(title)\n",
        "        resp = requests.get(f\"https://api.crossref.org/works?query.title={q}&rows=3\", timeout=15).json()\n",
        "\n",
        "        if resp.get(\"message\", {}).get(\"items\"):\n",
        "            for item in resp[\"message\"][\"items\"]:\n",
        "                abstract = clean_html(item.get(\"abstract\", \"\"))\n",
        "                if abstract and len(abstract) > 100:\n",
        "                    return abstract\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ CrossRef abstract search failed: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def get_semantic_scholar_abstract(title):\n",
        "    \"\"\"é€šè¿‡æ ‡é¢˜åœ¨Semantic Scholarä¸­æœç´¢æ‘˜è¦\"\"\"\n",
        "    try:\n",
        "        q = urllib.parse.quote(title)\n",
        "        url = f\"https://api.semanticscholar.org/graph/v1/paper/search?query={q}&limit=3&fields=title,abstract\"\n",
        "\n",
        "        response = requests.get(url, timeout=15)\n",
        "        data = response.json()\n",
        "\n",
        "        for paper in data.get(\"data\", []):\n",
        "            abstract = clean_html(paper.get(\"abstract\", \"\"))\n",
        "            if abstract and len(abstract) > 100:\n",
        "                return abstract\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Semantic Scholar abstract search failed: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def get_google_scholar_abstract(title):\n",
        "    \"\"\"é€šè¿‡Google Scholaræœç´¢æ‘˜è¦\"\"\"\n",
        "    try:\n",
        "        q = urllib.parse.quote(title)\n",
        "        url = f\"https://scholar.google.com/scholar?q={q}\"\n",
        "\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "\n",
        "        response = requests.get(url, headers=headers, timeout=15)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        abstract_div = soup.find('div', class_='gs_rs')\n",
        "        if abstract_div:\n",
        "            abstract = clean_html(abstract_div.text)\n",
        "            if abstract and len(abstract) > 100:\n",
        "                return abstract\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Google Scholar abstract search failed: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def get_springer_abstract(title):\n",
        "    \"\"\"é€šè¿‡SpringerLinkæœç´¢æ‘˜è¦\"\"\"\n",
        "    try:\n",
        "        q = urllib.parse.quote(title)\n",
        "        url = f\"https://api.springernature.com/metadata/json?q=title:{q}&api_key=demo&p=3\"\n",
        "\n",
        "        response = requests.get(url, timeout=15)\n",
        "        data = response.json()\n",
        "\n",
        "        for record in data.get(\"records\", []):\n",
        "            abstract = clean_html(record.get(\"abstract\", \"\"))\n",
        "            if abstract and len(abstract) > 100:\n",
        "                return abstract\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Springer abstract search failed: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def force_complete_abstract(item):\n",
        "    \"\"\"å¼ºåˆ¶è¡¥å…¨æ‘˜è¦ï¼Œä½¿ç”¨å¤šä¸ªæ•°æ®æºï¼ŒåŒ…æ‹¬DOIç½‘é¡µ\"\"\"\n",
        "    original_title = item[\"Title\"]\n",
        "    doi = item.get(\"DOI\", \"\")\n",
        "\n",
        "    search_sources = [\n",
        "        (\"PubMed\", get_pubmed_abstract),\n",
        "        (\"CrossRef\", get_crossref_abstract),\n",
        "        (\"Semantic Scholar\", get_semantic_scholar_abstract),\n",
        "        (\"Springer\", get_springer_abstract),\n",
        "        (\"Google Scholar\", get_google_scholar_abstract),\n",
        "    ]\n",
        "\n",
        "    # å¦‚æœæœ‰DOIï¼Œæ·»åŠ DOIç›¸å…³æœç´¢æº\n",
        "    if doi:\n",
        "        search_sources.insert(0, (\"DOI Publisher\", lambda title: get_publisher_abstract(doi)))\n",
        "        search_sources.insert(1, (\"DOI Webpage\", lambda title: extract_abstract_from_doi_webpage(doi)))\n",
        "\n",
        "    best_abstract = \"\"\n",
        "    best_source = \"Original\"\n",
        "    best_length = 0\n",
        "\n",
        "    if item[\"Abstract_En\"] and len(item[\"Abstract_En\"]) > 100:\n",
        "        best_abstract = item[\"Abstract_En\"]\n",
        "        best_source = item[\"Source\"]\n",
        "        best_length = len(best_abstract)\n",
        "\n",
        "    for source_name, search_func in search_sources:\n",
        "        if best_length > 400:\n",
        "            break\n",
        "\n",
        "        print(f\"  ğŸ” Trying {source_name}...\")\n",
        "        try:\n",
        "            if \"DOI\" in source_name:\n",
        "                # DOIç›¸å…³çš„æœç´¢ä¸éœ€è¦æ ‡é¢˜å‚æ•°\n",
        "                abstract = search_func(original_title) if \"Publisher\" in source_name else search_func(doi)\n",
        "            else:\n",
        "                abstract = search_func(original_title)\n",
        "\n",
        "            if abstract and len(abstract) > best_length + 50:\n",
        "                best_abstract = abstract\n",
        "                best_source = source_name\n",
        "                best_length = len(abstract)\n",
        "                print(f\"    âœ… Found better abstract from {source_name} ({len(abstract)} chars)\")\n",
        "\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    âš ï¸ {source_name} search error: {e}\")\n",
        "            continue\n",
        "\n",
        "    # å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„æ‘˜è¦\n",
        "    if not best_abstract or len(best_abstract) < 50:\n",
        "        if item[\"Abstract_En\"] and len(item[\"Abstract_En\"]) > 10:\n",
        "            best_abstract = item[\"Abstract_En\"]\n",
        "            best_source = f\"{item['Source']} (Short)\"\n",
        "        else:\n",
        "            best_abstract = \"Abstract not available from any source.\"\n",
        "            best_source = \"Not Found\"\n",
        "\n",
        "    return best_abstract, best_source\n",
        "\n",
        "def enhanced_abstract_completion(final_data):\n",
        "    \"\"\"å¢å¼ºçš„æ‘˜è¦è¡¥å…¨ï¼ŒåŒ…å«DOIç½‘é¡µæŠ“å–\"\"\"\n",
        "    print(\"\\nğŸ” ENHANCED ABSTRACT COMPLETION WITH DOI WEBPAGE EXTRACTION...\")\n",
        "    completion_stats = {\"improved\": 0, \"unchanged\": 0, \"failed\": 0, \"doi_extracted\": 0}\n",
        "\n",
        "    for i, d in enumerate(tqdm(final_data, desc=\"Abstract Completion\")):\n",
        "        if i % 5 == 0:  # æ›´é¢‘ç¹åœ°æ˜¾ç¤ºè¿›åº¦\n",
        "            print(f\"\\n[{i+1}/{len(final_data)}] Processing: {d['Title'][:80]}...\")\n",
        "            print(f\"  Source: {d['Source']}, DOI: {d.get('DOI', 'No DOI')}\")\n",
        "            print(f\"  Original abstract: {len(d['Abstract_En'])} chars\")\n",
        "\n",
        "        original_abstract = d[\"Abstract_En\"]\n",
        "        new_abstract, new_source = force_complete_abstract(d)\n",
        "\n",
        "        d[\"Abstract_En\"] = new_abstract\n",
        "        d[\"Abstract_Source\"] = new_source\n",
        "\n",
        "        # ç»Ÿè®¡\n",
        "        if \"DOI\" in new_source and len(new_abstract) > len(original_abstract) + 50:\n",
        "            completion_stats[\"doi_extracted\"] += 1\n",
        "            completion_stats[\"improved\"] += 1\n",
        "            print(f\"    ğŸ¯ DOI webpage extraction successful!\")\n",
        "        elif len(new_abstract) > len(original_abstract) + 100:\n",
        "            completion_stats[\"improved\"] += 1\n",
        "        elif new_source == \"Not Found\":\n",
        "            completion_stats[\"failed\"] += 1\n",
        "        else:\n",
        "            completion_stats[\"unchanged\"] += 1\n",
        "\n",
        "        # æ˜¾ç¤ºå½“å‰ç»“æœ\n",
        "        if i % 5 == 0:\n",
        "            print(f\"  Final abstract: {len(new_abstract)} chars from {new_source}\")\n",
        "\n",
        "        # ä¿å­˜è¿›åº¦\n",
        "        if i % 10 == 0:\n",
        "            save_progress('abstract_completion', final_data)\n",
        "\n",
        "    print(f\"\\nğŸ“Š ENHANCED ABSTRACT COMPLETION STATISTICS:\")\n",
        "    print(f\"  Improved: {completion_stats['improved']}\")\n",
        "    print(f\"  DOIç½‘é¡µæŠ“å–æˆåŠŸ: {completion_stats['doi_extracted']}\")\n",
        "    print(f\"  Unchanged: {completion_stats['unchanged']}\")\n",
        "    print(f\"  Failed: {completion_stats['failed']}\")\n",
        "\n",
        "    return completion_stats\n",
        "\n",
        "# ---------- SEARCH MODULES ----------\n",
        "def search_crossref():\n",
        "    print(\"\\nğŸ”¹ Searching CrossRef ...\")\n",
        "    url = \"https://api.crossref.org/works\"\n",
        "    params = {\"query\": \" \".join(KEYWORDS), \"filter\": f\"from-pub-date:{START_YEAR},until-pub-date:{END_YEAR}\", \"rows\": MAX_RESULTS_PER_SOURCE}\n",
        "    try:\n",
        "        data = requests.get(url, params=params, timeout=30).json()\n",
        "        items = []\n",
        "        for i in data.get(\"message\", {}).get(\"items\", []):\n",
        "            abs_text = clean_html(i.get(\"abstract\", \"\"))\n",
        "            item = {\n",
        "                \"Title\": i.get(\"title\", [\"\"])[0] if i.get(\"title\") else \"No title\",\n",
        "                \"Authors\": \", \".join([a.get(\"family\", \"\") for a in i.get(\"author\", [])]) if \"author\" in i else \"\",\n",
        "                \"Year\": i.get(\"created\", {}).get(\"date-parts\", [[None]])[0][0] if i.get(\"created\") else \"\",\n",
        "                \"DOI\": i.get(\"DOI\", \"\"),\n",
        "                \"Link\": doi_to_link(i.get(\"DOI\", \"\")),\n",
        "                \"Abstract_En\": abs_text,\n",
        "                \"Abstract_Source\": \"CrossRef\" if abs_text else \"No Abstract\",\n",
        "                \"Source\": \"CrossRef\"\n",
        "            }\n",
        "            items.append(item)\n",
        "        print(f\"âœ… CrossRef found {len(items)} results\")\n",
        "        return items\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ CrossRef search failed: {e}\")\n",
        "        return []\n",
        "\n",
        "def search_pubmed():\n",
        "    print(\"\\nğŸ”¹ Searching PubMed ...\")\n",
        "    try:\n",
        "        base = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
        "        term = \"+AND+\".join(KEYWORDS)\n",
        "        esearch = f\"{base}esearch.fcgi?db=pubmed&term={term}&retmax={MAX_RESULTS_PER_SOURCE}&mindate={START_YEAR}&maxdate={END_YEAR}&retmode=json\"\n",
        "\n",
        "        response = requests.get(esearch, timeout=30)\n",
        "        data = response.json()\n",
        "        ids = data.get(\"esearchresult\", {}).get(\"idlist\", [])\n",
        "\n",
        "        items = []\n",
        "        for pmid in tqdm(ids, desc=\"PubMed Progress\"):\n",
        "            try:\n",
        "                xml = requests.get(f\"{base}efetch.fcgi?db=pubmed&id={pmid}&retmode=xml\", timeout=15).text\n",
        "                soup = BeautifulSoup(xml, \"xml\")\n",
        "\n",
        "                title_elem = soup.find(\"ArticleTitle\")\n",
        "                title = title_elem.text if title_elem else \"No title\"\n",
        "\n",
        "                authors = \", \".join([a.text for a in soup.find_all(\"LastName\") if a.text])\n",
        "\n",
        "                year_tag = soup.find(\"PubDate\")\n",
        "                year = \"\"\n",
        "                if year_tag:\n",
        "                    year_elem = year_tag.find(\"Year\")\n",
        "                    year = year_elem.text if year_elem else \"\"\n",
        "\n",
        "                abs_text = clean_html(\" \".join([a.text for a in soup.find_all(\"AbstractText\")]))\n",
        "\n",
        "                items.append({\n",
        "                    \"Title\": title,\n",
        "                    \"Authors\": authors,\n",
        "                    \"Year\": year,\n",
        "                    \"DOI\": \"\",\n",
        "                    \"Link\": f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}/\",\n",
        "                    \"Abstract_En\": abs_text,\n",
        "                    \"Abstract_Source\": \"PubMed\" if abs_text else \"No Abstract\",\n",
        "                    \"Source\": \"PubMed\"\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Error processing PubMed ID {pmid}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"âœ… PubMed found {len(items)} results\")\n",
        "        return items\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ PubMed search failed: {e}\")\n",
        "        return []\n",
        "\n",
        "def search_semantic():\n",
        "    print(\"\\nğŸ”¹ Searching Semantic Scholar ...\")\n",
        "    try:\n",
        "        url = f\"https://api.semanticscholar.org/graph/v1/paper/search?query={'+'.join(KEYWORDS)}&year={START_YEAR}-{END_YEAR}&limit={MAX_RESULTS_PER_SOURCE}&fields=title,abstract,authors,year,externalIds,url\"\n",
        "        response = requests.get(url, timeout=30)\n",
        "        r = response.json()\n",
        "\n",
        "        items = []\n",
        "        for p in r.get(\"data\", []):\n",
        "            doi = p.get(\"externalIds\", {}).get(\"DOI\", \"\")\n",
        "            abs_text = clean_html(p.get(\"abstract\", \"\"))\n",
        "            authors = \", \".join([a[\"name\"] for a in p.get(\"authors\", [])]) if p.get(\"authors\") else \"\"\n",
        "\n",
        "            items.append({\n",
        "                \"Title\": p.get(\"title\", \"No title\"),\n",
        "                \"Authors\": authors,\n",
        "                \"Year\": p.get(\"year\", \"\"),\n",
        "                \"DOI\": doi,\n",
        "                \"Link\": doi_to_link(doi) if doi else p.get(\"url\", \"\"),\n",
        "                \"Abstract_En\": abs_text,\n",
        "                \"Abstract_Source\": \"SemanticScholar\" if abs_text else \"No Abstract\",\n",
        "                \"Source\": \"SemanticScholar\"\n",
        "            })\n",
        "        print(f\"âœ… Semantic Scholar found {len(items)} results\")\n",
        "        return items\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Semantic Scholar search failed: {e}\")\n",
        "        return []\n",
        "\n",
        "def search_high_impact_journals():\n",
        "    \"\"\"æœç´¢é«˜å½±å“å› å­æœŸåˆŠ - æ¯ä¸ªæœŸåˆŠ10ç¯‡æ–‡çŒ®\"\"\"\n",
        "    print(\"\\nğŸ”¹ Searching High-Impact Journals (10 articles per journal)...\")\n",
        "    items = []\n",
        "\n",
        "    # é€‰é¡¹3ï¼šæŒ‰åˆ†ç±»æœç´¢ï¼ˆæ›´æ™ºèƒ½çš„æ–¹å¼ï¼‰\n",
        "    journals_to_search = (\n",
        "         HIGH_IMPACT_JOURNALS[:30] +  # é¡¶çº§ç»¼åˆæœŸåˆŠ\n",
        "         HIGH_IMPACT_JOURNALS[30:130] +  # ç²¾ç¥ç—…å­¦ä¸ç¥ç»ç§‘å­¦\n",
        "         HIGH_IMPACT_JOURNALS[130:145] +  # å†…åˆ†æ³Œä¸ä»£è°¢\n",
        "         HIGH_IMPACT_JOURNALS[210:225] +  # è¯ç†å­¦ä¸è¯ç‰©å‘ç°\n",
        "         HIGH_IMPACT_JOURNALS[280:295] +  # å¿ƒè¡€ç®¡ç§‘å­¦\n",
        "         HIGH_IMPACT_JOURNALS[340:355]    # åˆ†å­ç”Ÿç‰©å­¦ä¸ç»†èƒç”Ÿç‰©å­¦\n",
        "    )\n",
        "\n",
        "    for journal in tqdm(journals_to_search, desc=\"High-Impact Journals\"):\n",
        "        try:\n",
        "            query = f\"({' '.join(KEYWORDS)}) AND ({journal}[Journal])\"\n",
        "            url = \"https://api.crossref.org/works\"\n",
        "            params = {\n",
        "                \"query\": query,\n",
        "                \"filter\": f\"from-pub-date:{START_YEAR},until-pub-date:{END_YEAR}\",\n",
        "                \"rows\": 10  # æ¯ä¸ªæœŸåˆŠæœç´¢10ç¯‡\n",
        "            }\n",
        "\n",
        "            response = requests.get(url, params=params, timeout=25)\n",
        "            data = response.json()\n",
        "\n",
        "            journal_items = []\n",
        "            for item in data.get(\"message\", {}).get(\"items\", []):\n",
        "                container_title = item.get(\"container-title\", [\"\"])[0]\n",
        "                if journal.lower() in container_title.lower():\n",
        "                    abs_text = clean_html(item.get(\"abstract\", \"\"))\n",
        "                    journal_items.append({\n",
        "                        \"Title\": item.get(\"title\", [\"\"])[0] if item.get(\"title\") else \"No title\",\n",
        "                        \"Authors\": \", \".join([a.get(\"family\", \"\") for a in item.get(\"author\", [])]) if \"author\" in item else \"\",\n",
        "                        \"Year\": item.get(\"created\", {}).get(\"date-parts\", [[None]])[0][0] if item.get(\"created\") else \"\",\n",
        "                        \"DOI\": item.get(\"DOI\", \"\"),\n",
        "                        \"Link\": doi_to_link(item.get(\"DOI\", \"\")),\n",
        "                        \"Abstract_En\": abs_text,\n",
        "                        \"Abstract_Source\": f\"High-Impact: {journal}\" if abs_text else \"No Abstract\",\n",
        "                        \"Source\": f\"High-Impact: {journal}\",\n",
        "                        \"Journal\": journal\n",
        "                    })\n",
        "\n",
        "            items.extend(journal_items)\n",
        "            print(f\"  ğŸ“š {journal}: found {len(journal_items)} articles\")\n",
        "\n",
        "            time.sleep(1.5)  # å¢åŠ å»¶è¿Ÿé¿å…è¢«å°\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Error searching {journal}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"âœ… High-Impact Journals found {len(items)} results from {len(journals_to_search)} journals\")\n",
        "\n",
        "    # æ˜¾ç¤ºæœŸåˆŠåˆ†ç±»ç»Ÿè®¡\n",
        "    journal_stats = {}\n",
        "    category_stats = {\n",
        "        \"é¡¶çº§ç»¼åˆæœŸåˆŠ\": 0,\n",
        "        \"ç²¾ç¥ç—…å­¦ä¸ç¥ç»ç§‘å­¦\": 0,\n",
        "        \"å†…åˆ†æ³Œä¸ä»£è°¢\": 0,\n",
        "        \"è¯ç†å­¦ä¸è¯ç‰©å‘ç°\": 0,\n",
        "        \"å¿ƒè¡€ç®¡ç§‘å­¦\": 0,\n",
        "        \"åˆ†å­ç”Ÿç‰©å­¦ä¸ç»†èƒç”Ÿç‰©å­¦\": 0\n",
        "    }\n",
        "\n",
        "    for item in items:\n",
        "        journal = item[\"Journal\"]\n",
        "        journal_stats[journal] = journal_stats.get(journal, 0) + 1\n",
        "\n",
        "        # ç»Ÿè®¡å„åˆ†ç±»çš„æ–‡çŒ®æ•°é‡\n",
        "        if journal in HIGH_IMPACT_JOURNALS[:30]:\n",
        "            category_stats[\"é¡¶çº§ç»¼åˆæœŸåˆŠ\"] += 1\n",
        "        elif journal in HIGH_IMPACT_JOURNALS[30:130]:\n",
        "            category_stats[\"ç²¾ç¥ç—…å­¦ä¸ç¥ç»ç§‘å­¦\"] += 1\n",
        "        elif journal in HIGH_IMPACT_JOURNALS[130:210]:\n",
        "            category_stats[\"å†…åˆ†æ³Œä¸ä»£è°¢\"] += 1\n",
        "        elif journal in HIGH_IMPACT_JOURNALS[210:280]:\n",
        "            category_stats[\"è¯ç†å­¦ä¸è¯ç‰©å‘ç°\"] += 1\n",
        "        elif journal in HIGH_IMPACT_JOURNALS[280:340]:\n",
        "            category_stats[\"å¿ƒè¡€ç®¡ç§‘å­¦\"] += 1\n",
        "        elif journal in HIGH_IMPACT_JOURNALS[340:]:\n",
        "            category_stats[\"åˆ†å­ç”Ÿç‰©å­¦ä¸ç»†èƒç”Ÿç‰©å­¦\"] += 1\n",
        "\n",
        "    print(f\"ğŸ“Š æœŸåˆŠåˆ†ç±»åˆ†å¸ƒ:\")\n",
        "    for category, count in category_stats.items():\n",
        "        if count > 0:\n",
        "            print(f\"   {category}: {count} ç¯‡æ–‡çŒ®\")\n",
        "\n",
        "    return items\n",
        "\n",
        "def search_springer():\n",
        "    \"\"\"æœç´¢SpringerLink\"\"\"\n",
        "    print(\"\\nğŸ”¹ Searching Springer Link ...\")\n",
        "    try:\n",
        "        query = \"+AND+\".join(KEYWORDS)\n",
        "        url = f\"https://api.springernature.com/metadata/json?q={query}&api_key=demo&p={MAX_RESULTS_PER_SOURCE//2}\"\n",
        "\n",
        "        response = requests.get(url, timeout=30)\n",
        "        data = response.json()\n",
        "\n",
        "        items = []\n",
        "        for record in data.get(\"records\", []):\n",
        "            items.append({\n",
        "                \"Title\": record.get(\"title\", \"No title\"),\n",
        "                \"Authors\": \", \".join([creator.get(\"creator\", \"\") for creator in record.get(\"creators\", [])]),\n",
        "                \"Year\": record.get(\"publicationDate\", \"\").split(\"-\")[0] if record.get(\"publicationDate\") else \"\",\n",
        "                \"DOI\": record.get(\"doi\", \"\"),\n",
        "                \"Link\": f\"https://link.springer.com/article/{record.get('doi', '')}\",\n",
        "                \"Abstract_En\": clean_html(record.get(\"abstract\", \"\")),\n",
        "                \"Abstract_Source\": \"Springer\" if record.get(\"abstract\") else \"No Abstract\",\n",
        "                \"Source\": \"Springer\"\n",
        "            })\n",
        "\n",
        "        print(f\"âœ… Springer found {len(items)} results\")\n",
        "        return items\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Springer search failed: {e}\")\n",
        "        return []\n",
        "\n",
        "# ---------- ENHANCED SEARCH WITH KEYWORD FILTERING ----------\n",
        "def search_all_sources_with_filtering():\n",
        "    \"\"\"æœç´¢æ‰€æœ‰æ•°æ®æºå¹¶è¿›è¡Œå…³é”®è¯ç­›é€‰\"\"\"\n",
        "    print(\"ğŸš€ Starting Enhanced Literature Collection with Keyword Filtering\")\n",
        "\n",
        "    # æ£€æŸ¥æ˜¯å¦æœ‰å·²ä¿å­˜çš„æ•°æ®\n",
        "    progress, saved_data = check_existing_data()\n",
        "\n",
        "    if progress and progress['stage'] == 'search_complete':\n",
        "        print(\"âœ… Using saved search results\")\n",
        "        all_items = saved_data['search_results']\n",
        "    else:\n",
        "        all_items = []\n",
        "        search_functions = [\n",
        "            search_crossref,\n",
        "            search_pubmed,\n",
        "            search_semantic,\n",
        "            search_high_impact_journals,\n",
        "            search_springer\n",
        "        ]\n",
        "\n",
        "        for func in search_functions:\n",
        "            try:\n",
        "                results = func()\n",
        "                all_items.extend(results)\n",
        "                print(f\"ğŸ“¥ Added {len(results)} from {func.__name__}\")\n",
        "\n",
        "                # æ¯æ¬¡æœç´¢åéƒ½ä¿å­˜è¿›åº¦\n",
        "                save_progress('searching', all_items)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Error in {func.__name__}: {e}\")\n",
        "\n",
        "        # æœç´¢å®Œæˆï¼Œä¿å­˜æœ€ç»ˆç»“æœ\n",
        "        save_progress('search_complete', all_items)\n",
        "\n",
        "    # å…³é”®è¯æ‰©å±•å’Œç­›é€‰\n",
        "    expanded_keywords = smart_keyword_expansion(KEYWORDS)\n",
        "    filtered_items = filter_by_keywords(all_items, expanded_keywords, min_relevance_score=1.5)\n",
        "\n",
        "    # åˆ†æå…³é”®è¯åˆ†å¸ƒ\n",
        "    analyze_keyword_distribution(filtered_items, expanded_keywords)\n",
        "\n",
        "    return filtered_items, expanded_keywords\n",
        "\n",
        "# ---------- MODIFIED DOCUMENT GENERATION ----------\n",
        "def generate_word_document(final_data, completion_stats, expanded_keywords=None, all_items_count=0):\n",
        "    \"\"\"ç”ŸæˆWordæ–‡æ¡£ï¼ŒåŒ…å«å…³é”®è¯ä¿¡æ¯\"\"\"\n",
        "    doc = Document()\n",
        "\n",
        "    # æ·»åŠ æ–‡æ¡£æ ‡é¢˜\n",
        "    doc.add_heading(f'æ–‡çŒ®æ£€ç´¢æŠ¥å‘Š: {\" \".join(KEYWORDS)}', 0)\n",
        "    doc.add_paragraph(f'æ£€ç´¢æ—¶é—´: {datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
        "    doc.add_paragraph(f'æ£€ç´¢å…³é”®è¯: {\", \".join(KEYWORDS)}')\n",
        "\n",
        "    if expanded_keywords:\n",
        "        doc.add_paragraph(f'æ‰©å±•å…³é”®è¯: {\", \".join(expanded_keywords)}')\n",
        "\n",
        "    doc.add_paragraph(f'æ—¶é—´èŒƒå›´: {START_YEAR}-{END_YEAR}')\n",
        "    doc.add_paragraph(f'æ€»è®¡ç›¸å…³æ–‡çŒ®: {len(final_data)} ç¯‡')\n",
        "\n",
        "    # ä½¿ç”¨ä¼ é€’çš„å‚æ•°\n",
        "    if all_items_count > 0:\n",
        "        removed_count = all_items_count - len(final_data)\n",
        "        doc.add_paragraph(f'å»é‡ç»Ÿè®¡: åˆå§‹ {all_items_count} ç¯‡ â†’ å»é‡å {len(final_data)} ç¯‡ (ç§»é™¤ {removed_count} ç¯‡é‡å¤)')\n",
        "    else:\n",
        "        doc.add_paragraph(f'å»é‡ç»Ÿè®¡: å»é‡å {len(final_data)} ç¯‡æ–‡çŒ®')\n",
        "\n",
        "    doc.add_paragraph(f'æ‘˜è¦è¡¥å…¨: æ”¹è¿› {completion_stats[\"improved\"]} ç¯‡, æœªå˜ {completion_stats[\"unchanged\"]} ç¯‡, å¤±è´¥ {completion_stats[\"failed\"]} ç¯‡')\n",
        "\n",
        "    # æ·»åŠ ç›¸å…³æ€§åˆ†æ•°ç»Ÿè®¡\n",
        "    relevance_scores = [item.get(\"Relevance_Score\", 0) for item in final_data]\n",
        "    if relevance_scores and len(relevance_scores) > 0:\n",
        "        avg_relevance = sum(relevance_scores) / len(relevance_scores)\n",
        "        doc.add_paragraph(f'å¹³å‡ç›¸å…³æ€§åˆ†æ•°: {avg_relevance:.2f}')\n",
        "\n",
        "    doc.add_paragraph()\n",
        "\n",
        "    # é«˜å½±å“å› å­æ–‡çŒ®ç‰¹åˆ«æ˜¾ç¤º\n",
        "    high_impact_items = [d for d in final_data if \"High-Impact\" in d.get(\"Source\", \"\")]\n",
        "    if high_impact_items:\n",
        "        doc.add_heading('ğŸ¯ é«˜å½±å“å› å­æ–‡çŒ®', level=1)\n",
        "        doc.add_paragraph(f'å…±æ‰¾åˆ° {len(high_impact_items)} ç¯‡æ¥è‡ªé«˜å½±å“å› å­æœŸåˆŠçš„æ–‡çŒ®')\n",
        "\n",
        "        for i, r in enumerate(high_impact_items, 1):\n",
        "            doc.add_heading(f\"{i}. {r['Title']}\", level=2)\n",
        "            doc.add_paragraph(f\"ğŸ‘¥ Authors: {r['Authors']}\")\n",
        "            doc.add_paragraph(f\"ğŸ“… Year: {r['Year']} | ğŸ“š Journal: {r['Source'].replace('High-Impact: ', '')}\")\n",
        "\n",
        "            if r[\"DOI\"]:\n",
        "                p = doc.add_paragraph(\"ğŸ”— DOI: \")\n",
        "                make_hyperlink(p, f\"https://doi.org/{r['DOI']}\", r[\"DOI\"])\n",
        "\n",
        "            doc.add_paragraph(\"ğŸ“„ Abstract (English):\")\n",
        "            doc.add_paragraph(r['Abstract_En'])\n",
        "            doc.add_paragraph(\"ğŸ“– æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰:\")\n",
        "            doc.add_paragraph(r['Abstract_Zh'])\n",
        "            doc.add_paragraph(\"-\" * 60)\n",
        "\n",
        "    # å…¶ä»–æ–‡çŒ®æŒ‰æ¥æºåˆ†ç»„\n",
        "    sources = sorted(list(set([d[\"Source\"] for d in final_data if \"High-Impact\" not in d[\"Source\"]])))\n",
        "    for source in sources:\n",
        "        source_items = [d for d in final_data if d[\"Source\"] == source]\n",
        "        if source_items:\n",
        "            doc.add_heading(f'ğŸ“– {source} ({len(source_items)}ç¯‡)', level=1)\n",
        "\n",
        "            for i, r in enumerate(source_items, 1):\n",
        "                doc.add_heading(f\"{i}. {r['Title']}\", level=2)\n",
        "                doc.add_paragraph(f\"ğŸ‘¥ Authors: {r['Authors']}\")\n",
        "                doc.add_paragraph(f\"ğŸ“… Year: {r['Year']} | ğŸ” Source: {r['Source']}\")\n",
        "\n",
        "                if r[\"DOI\"]:\n",
        "                    p = doc.add_paragraph(\"ğŸ”— DOI: \")\n",
        "                    make_hyperlink(p, f\"https://doi.org/{r['DOI']}\", r[\"DOI\"])\n",
        "                elif r[\"Link\"]:\n",
        "                    p = doc.add_paragraph(\"ğŸ”— Link: \")\n",
        "                    make_hyperlink(p, r[\"Link\"], \"Click to view original paper\")\n",
        "\n",
        "                doc.add_paragraph(\"ğŸ“„ Abstract (English):\")\n",
        "                doc.add_paragraph(r['Abstract_En'])\n",
        "                doc.add_paragraph(\"ğŸ“– æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰:\")\n",
        "                doc.add_paragraph(r['Abstract_Zh'])\n",
        "                doc.add_paragraph(\"-\" * 60)\n",
        "\n",
        "    return doc\n",
        "\n",
        "# ---------- MODIFIED MAIN FUNCTION ----------\n",
        "def main():\n",
        "    \"\"\"ä¸»æ‰§è¡Œå‡½æ•°ï¼ŒåŒ…å«å…³é”®è¯ç­›é€‰å’Œæ•°æ®æŒä¹…åŒ–\"\"\"\n",
        "\n",
        "    # é˜¶æ®µ1: æœç´¢æ–‡çŒ®å¹¶è¿›è¡Œå…³é”®è¯ç­›é€‰\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ğŸ“š STAGE 1: Literature Search with Keyword Filtering\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    all_items, expanded_keywords = search_all_sources_with_filtering()\n",
        "    print(f\"âœ… Search and filtering completed: {len(all_items)} relevant items found\")\n",
        "\n",
        "    # é˜¶æ®µ2: å»é‡\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ğŸ”„ STAGE 2: Deduplication\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    progress, saved_data = check_existing_data()\n",
        "    if progress and progress['stage'] == 'deduplication_complete':\n",
        "        print(\"âœ… Using saved deduplicated data\")\n",
        "        final_data = saved_data['final_data']\n",
        "    else:\n",
        "        print(\"ğŸ”¹ Step 1: Basic deduplication...\")\n",
        "        basic_dedup_items = group_and_deduplicate(all_items)\n",
        "\n",
        "        print(\"ğŸ”¹ Step 2: Advanced similarity deduplication...\")\n",
        "        final_data = advanced_deduplication(basic_dedup_items)\n",
        "\n",
        "        save_progress('deduplication_complete', final_data)\n",
        "\n",
        "    print(f\"âœ… Deduplication completed: {len(final_data)} unique relevant items\")\n",
        "\n",
        "    # é˜¶æ®µ3: å¢å¼ºæ‘˜è¦è¡¥å…¨\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ğŸŒ STAGE 3: Enhanced Abstract Completion\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    completion_stats = enhanced_abstract_completion(final_data)\n",
        "\n",
        "\n",
        "     # ---------- TRANSLATION ----------\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ğŸˆº STAGE 4: Translation\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    for idx, item in enumerate(tqdm(final_data, desc=\"Translation Progress\")):\n",
        "        if item[\"Abstract_En\"] and item[\"Abstract_En\"] != \"Abstract not available from any source.\":\n",
        "            item[\"Abstract_Zh\"] = translate_to_cn(item[\"Abstract_En\"])\n",
        "        else:\n",
        "            item[\"Abstract_Zh\"] = \"æœªæ‰¾åˆ°æ‘˜è¦\"\n",
        "\n",
        "    # æ¯ç¿»è¯‘10ä¸ªæ–‡çŒ®ä¿å­˜ä¸€æ¬¡è¿›åº¦\n",
        "        if idx % 10 == 0:\n",
        "            save_progress('translation', final_data)\n",
        "\n",
        "# é˜¶æ®µ5: ç”Ÿæˆæ–‡æ¡£\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ğŸ“„ STAGE 5: Document Generation\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    output_name = f\"{FILE_BASENAME}.docx\"\n",
        "    doc = generate_word_document(final_data, completion_stats, expanded_keywords, len(all_items))\n",
        "    doc.save(output_name)\n",
        "\n",
        "    # ä¿å­˜åˆ°Google Driveï¼ˆå¯é€‰ï¼‰\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        drive_path = f\"/content/drive/MyDrive/{output_name}\"\n",
        "        doc.save(drive_path)\n",
        "        print(f\"âœ… Also saved to Google Drive: {drive_path}\")\n",
        "    except:\n",
        "        print(\"â„¹ï¸ Google Drive not available\")\n",
        "\n",
        "    # æœ€ç»ˆç»Ÿè®¡\n",
        "    print(f\"\\nğŸ“Š FINAL STATISTICS:\")\n",
        "    print(f\"   Unique relevant items: {len(final_data)}\")\n",
        "    high_impact_items = [d for d in final_data if \"High-Impact\" in d.get(\"Source\", \"\")]\n",
        "    print(f\"   High-impact items: {len(high_impact_items)}\")\n",
        "    print(f\"   Items with DOI: {len([d for d in final_data if d['DOI']])}\")\n",
        "    print(f\"   Items with full abstract: {len([d for d in final_data if len(d['Abstract_En']) > 200])}\")\n",
        "\n",
        "    # ä¸‹è½½æ–‡ä»¶\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        files.download(output_name)\n",
        "        print(f\"ğŸ“¥ File '{output_name}' downloaded successfully!\")\n",
        "    except:\n",
        "        print(\"ğŸ’» File saved in current directory\")\n",
        "\n",
        "    # ä¿å­˜å®ŒæˆçŠ¶æ€\n",
        "    save_progress('complete', final_data)\n",
        "    print(f\"\\nğŸ‰ Literature collection completed successfully!\")\n",
        "    print(f\"ğŸ’¾ All data saved in: {DATA_DIR}\")\n",
        "\n",
        "# ---------- æ‰§è¡Œä¸»å‡½æ•° ----------\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "UVO-FYqtGHmb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
